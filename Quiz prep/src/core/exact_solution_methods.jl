using LinearAlgebra

# MDP lookahead
function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    #println("Calculating lookahead for state: $s, action: $a")
    reward = R(s, a)
    #println("Immediate reward: $reward")
    future_value = Î³ * sum(T(s, a, sâ€²) * U[i] for (i, sâ€²) in enumerate(ğ’®))
    #println("Future value: $future_value")
    return reward + future_value
end


################
# iterative policy evaluation: computes the utilility of a policy Ï€ 
################
function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end

################
# policy evaluation: computes the utilility of a policy Ï€ 
################
function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³ * Tâ€²) \ Râ€²
end

# helper function to compute the value function
function compute_value_function(T::Matrix{Float64}, R::Vector{Float64}, Î³::Float64)
    #I = Matrix{Float64}(I, size(T, 1), size(T, 2))
    return (I - Î³ * T) \ R
end

################
# value function policy: A value function policy extracted from a value function U 
#for an MDP ğ’«. The greedy function will be used in other algorithms.
################
struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a -> lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end
(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a



################
# policy iteration: computes the optimal policy for an MDP ğ’«
################
struct PolicyIteration
    Ï€ # initial policy
    k_max # maximum number of iterations
end

function solve(M::PolicyIteration, ğ’«::MDP)
    Ï€, ğ’® = M.Ï€, ğ’«.ğ’®
    for k = 1:M.k_max
        U = policy_evaluation(ğ’«, Ï€)
        Ï€â€² = ValueFunctionPolicy(ğ’«, U)
        if all(Ï€(s) == Ï€â€²(s) for s in ğ’®)
            break
        end
        Ï€ = Ï€â€²
    end
    return Ï€
end


################
# value iteration: computes the optimal policy for an MDP ğ’«
################
function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end

struct ValueIteration
    k_max # maximum number of iterations
end

function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end

export ValueIteration, PolicyIteration, solve, ValueFunctionPolicy, iterative_policy_evaluation, policy_evaluation



