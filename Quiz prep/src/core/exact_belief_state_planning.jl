using JuMP, GLPK

# belief update: using bayes' rule to update the belief state   
function update(b::Vector{Float64}, ğ’«, a, o)
    ğ’®, T, O = ğ’«.ğ’®, ğ’«.T, ğ’«.O
    bâ€² = similar(b)
    for (iâ€², sâ€²) in enumerate(ğ’®)
        po = O(a, sâ€², o)
        bâ€²[iâ€²] = po * sum(T(s, a, sâ€²) * b[i] for (i, s) in enumerate(ğ’®))
    end
    if sum(bâ€²) â‰ˆ 0.0
        fill!(bâ€², 1)
    end
    return normalize!(bâ€², 1)
end

# lookahead for a POMDP 
function lookahead(ğ’«::POMDP, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s, a, sâ€²) * sum(O(a, sâ€², o) * U(o, sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s, a) + Î³ * uâ€²
end

################
# conditional plan
################
struct ConditionalPlan
    a # action to take at root
    subplans # dictionary mapping observations to subplans
end

ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]

function evaluate_plan(ğ’«::POMDP, Ï€::ConditionalPlan, s)
    U(o, sâ€²) = evaluate_plan(ğ’«, Ï€(o), sâ€²)
    return isempty(Ï€.subplans) ? ğ’«.R(s, Ï€()) : lookahead(ğ’«, U, s, Ï€())
end


###############
# alpha vectors
###############
function alphavector(ğ’«::POMDP, Ï€::ConditionalPlan)
    return [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
end
# A policy that selects actions based on the alpha vector
struct AlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
    a # actions associated with alpha vectors
end
function utility(Ï€::AlphaVectorPolicy, b)
    return maximum(Î± â‹… b for Î± in Ï€.Î“)
end
function (Ï€::AlphaVectorPolicy)(b)
    i = argmax([Î± â‹… b for Î± in Ï€.Î“])
    return Ï€.a[i]
end

function optimal_valuefn(alpha, b)
    println("Dot products of alpha vectors with belief:")
    values = [Î± â‹… b for Î± in alpha]
    for (i, val) in enumerate(values)
        println("Î±$i â‹… b = $val")
    end
    return maximum(values)
end

###############
# one step lookahead
###############

function lookahead(ğ’«::POMDP, U, b::Vector, a)
    # println("LOOKAHEAD VALUES--------------")
    # println("Action: ", a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    r = sum(R(s, a) * b[i] for (i, s) in enumerate(ğ’®))
    # println("Reward: ", r)

    Posa(o, s, a) = sum(O(a, sâ€², o) * T(s, a, sâ€²) for sâ€² in ğ’®)
    Poba(o, b, a) = sum(b[i] * Posa(o, s, a) for (i, s) in enumerate(ğ’®))

    total = 0.0
    for o in ğ’ª
        prob = Poba(o, b, a)
        updated_belief = update(b, ğ’«, a, o)
        future_value = U(updated_belief)
        contribution = prob * future_value
        # println("Observation: ", o)
        # println("  P(o|b,a): ", prob)
        # println("  Updated belief: ", updated_belief)
        # println("  Future value: ", future_value)
        # println("  Contribution: ", Î³ * contribution)
        total += Î³ * contribution
    end
    # println("Total future reward: ", total)
    println("Total value: ", r + total)
    return r + total
end

function greedy(ğ’«::POMDP, U, b::Vector)
    u, a = findmax(a -> lookahead(ğ’«, U, b, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

struct LookaheadAlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
end

function utility(Ï€::LookaheadAlphaVectorPolicy, b)
    val = maximum(Î± â‹… b for Î± in Ï€.Î“)
    println("At belief $b, utility is $val")
    return val
end
function greedy(Ï€, b)
    U(b) = utility(Ï€, b)
    return greedy(Ï€.ğ’«, U, b)
end
(Ï€::LookaheadAlphaVectorPolicy)(b) = greedy(Ï€, b).a #remove a to see the utility


################
# pruning alpha vectors
################    
function find_maximal_belief(Î±, Î“)
    m = length(Î±)
    if isempty(Î“)
        return fill(1 / m, m) # arbitrary belief
    end
    model = Model(GLPK.Optimizer)
    @variable(model, Î´)
    @variable(model, b[i=1:m] â‰¥ 0)
    @constraint(model, sum(b) == 1.0)
    for a in Î“
        @constraint(model, (Î± - a) â‹… b â‰¥ Î´)
    end
    @objective(model, Max, Î´)
    optimize!(model)
    return value(Î´) > 0 ? value.(b) : nothing
end

function find_dominating(Î“)
    n = length(Î“)
    candidates, dominating = trues(n), falses(n)
    while any(candidates)
        i = findfirst(candidates)
        b = find_maximal_belief(Î“[i], Î“[dominating])
        if b === nothing
            candidates[i] = false
        else
            k = argmax([candidates[j] ? b â‹… Î“[j] : -Inf for j in 1:n])
            candidates[k], dominating[k] = false, true
        end
    end
    return dominating
end
function prune(plans, Î“)
    d = find_dominating(Î“)
    return (plans[d], Î“[d])
end



################
# value iteration
################
struct ValueIteration
    k_max # maximum number of iterations
end
function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end
function value_iteration(ğ’«::POMDP, k_max)
    ğ’®, ğ’œ, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    plans = [ConditionalPlan(a) for a in ğ’œ]
    Î“ = [[R(s, a) for s in ğ’®] for a in ğ’œ]
    plans, Î“ = prune(plans, Î“)
    for k in 2:k_max
        plans, Î“ = expand(plans, Î“, ğ’«)
        plans, Î“ = prune(plans, Î“)
    end
    return (plans, Î“)
end
function solve(M::ValueIteration, ğ’«::POMDP)
    plans, Î“ = value_iteration(ğ’«, M.k_max)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end



#####
# 22: forward search 
#####   
struct ForwardSearch
    ğ’« # problem
    d # depth
    U # value function at depth d
end
function forward_search(ğ’«, s, d, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end
    best = (a=nothing, u=-Inf)
    Uâ€²(s) = forward_search(ğ’«, s, d - 1, U).u
    for a in ğ’«.ğ’œ
        println("Action: $a at depth $d")
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::ForwardSearch)(s) = forward_search(Ï€.ğ’«, s, Ï€.d, Ï€.U).a




export LookaheadAlphaVectorPolicy, AlphaVectorPolicy, utility, greedy, optimal_valuefn